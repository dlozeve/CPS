#+TITLE: Simulation of the Brownian motion
#+AUTHOR: Dimitri Lozeve
#+EMAIL: dimitri.lozeve@polytechnique.edu

#+PROPERTY: header-args :tangle yes
#+HTML_MATHJAX:  path:"http://cdn.mathjax.org/mathjax/latest/MathJax.js"

#+BEGIN_SRC ipython :session  :exports both
  import numpy as np
  from scipy import stats
  import matplotlib
  matplotlib.use("Qt5Agg")
  import matplotlib.pyplot as plt
  plt.style.use("ggplot")
  %matplotlib inline
#+END_SRC

#+RESULTS:

* Forward simulation of \(\{W_{t_1^n}, \cdots, W_{t_n^n}\}\)

According to the definition of the Brownian motion, \(W_{t_i^n} -
W_{t_{i-1}^n} \sim N(0, t_i^n - t_{i-1}^n)\). Thus, there exists a
standard normal distributed random variable \(Z_i\) such that
\(W_{t_i^n} - W_{t_{i-1}^n} = Z_i \sqrt{\Delta t}\).
   
#+BEGIN_SRC ipython :session  :exports both
  def brownianmotion(T,n):
      """Returns a Brownian motion of size 2**n, with total time T.
      
      """
      # standard normal distributed random variables Z_i
      z = stats.norm.rvs(size=2**n)
      w = np.zeros(2**n)
      var = np.sqrt(2**(-n) * T)
      for i in range(1,2**n):
          w[i] = w[i-1] + z[i]*var
      return w
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session  :exports both
  # We compute 1000 copies of the discretized Brownian motion.

  res = np.zeros((1000,2**2))
  for j in range(0,1000):
      res[j] = brownianmotion(1,2)
  res
#+END_SRC

#+RESULTS:
: array([[ 0.        ,  0.18927533, -0.69478549, -1.61370099],
:        [ 0.        , -0.42221717, -0.83084678, -0.5598587 ],
:        [ 0.        , -0.72090478, -0.05071697, -0.00657082],
:        ..., 
:        [ 0.        ,  0.64648926,  0.9430492 ,  1.39470021],
:        [ 0.        ,  0.19206339,  1.30919734,  1.22523421],
:        [ 0.        , -0.10462534, -0.3285888 ,  1.02700974]])


#+BEGIN_SRC ipython :session  :exports both :results output
  print("E(W_T) =", np.mean(res[:,-1]))
  print("Var(W_T) =", np.var(res[:,-1]))
  print("Cov(W_T/2, W_T) =", np.cov(res[:,2**2/2], res[:,-1])[1,1])
#+END_SRC

#+RESULTS:
: E(W_T) = -0.0121066509394
: Var(W_T) = 0.741486188109
: Cov(W_T/2, W_T) = 0.742228416525

#+BEGIN_SRC ipython :session :exports both
  def meanvarcov(n, m):
      """Returns the sample mean and variance of W_T and covariance of
      W_T/2, W_T, using a Brownian motion of size 2**n and a sample of
      size m.

      """
      res = np.zeros((m,2**n))
      for j in range(0,m):
          res[j] = brownianmotion(1,n)
      mean = np.mean(res[:,-1])
      var = np.var(res[:,-1])
      cov = np.cov(res[:,2**n/2], res[:,-1])[1,1]
      return mean, var, cov
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports both
  mean = np.zeros(12)
  var = np.zeros(12)
  cov = np.zeros(12)
  for n in range(0,12):
      mean[n], var[n], cov[n] = meanvarcov(n, 1000)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file /home/dimitri/cours/3A/MAP552/CPS/CPS2/py526Gxt.png :exports both
  plt.plot(mean)
  plt.xlabel("n")
  plt.ylabel("Mean")
#+END_SRC

#+RESULTS:
[[file:/home/dimitri/cours/3A/MAP552/CPS/CPS2/py526Gxt.png]]
   
We can see on the plot that the mean remains very close to zero, as
expected, since \(W_T\) follows a centered normal distribution.


#+BEGIN_SRC ipython :session :file /home/dimitri/cours/3A/MAP552/CPS/CPS2/py526_Bu.png :exports both
  plt.plot(var)
  plt.xlabel("n")
  plt.ylabel("Variance")
#+END_SRC

#+RESULTS:
[[file:/home/dimitri/cours/3A/MAP552/CPS/CPS2/py526_Bu.png]]

#+BEGIN_SRC ipython :session :file /home/dimitri/cours/3A/MAP552/CPS/CPS2/py526MTo.png :exports both
  plt.plot(cov)
  plt.xlabel("n")
  plt.ylabel("Covariance")
#+END_SRC

#+RESULTS:
[[file:/home/dimitri/cours/3A/MAP552/CPS/CPS2/py526MTo.png]]

As expected from theoretical results, the variance and covariance of
the Brownian motion tend to $T$ when $n$ goes to $+\infty$.


* Backward simulation of \(\{W_{t_1^n}, \cdots, W_{t_n^n}\}\)

Let \(\bar{s} = \frac{s_1+s_2}{2}.\) Let us compute the Laplace
transform of \(W_\bar{s} \;|\; W_{s_1} = x_1, W_{s_2} = x_2.\)

\begin{align*}
\mathbb{E}\left(e^{u W_{\bar{s}}} \;|\; W_{s_1} = x_1, W_{s_2} = x_2\right)
&= \mathbb{E}\left(e^{u \left(W_{\bar{s}} - \frac{W_{s_1} + W_{s_2}}{2}\right)} e^{u \frac{W_{s_1} + W_{s_2}}{2}} \;|\; W_{s_1} = x_1, W_{s_2} = x_2\right) \\
&= e^{u \frac{x_1+x_2}{2}} \mathbb{E}\left(e^{u \left(W_{\bar{s}} - \frac{W_{s_1} + W_{s_2}}{2}\right)} \;|\; W_{s_1} = x_1, W_{s_2} = x_2\right) \\
&= e^{u \bar{x}} \mathbb{E}\left(\exp\left(\frac{1}{2}u\left(W_\bar{s}-W_{s_1}\right)\right) \exp\left(-\frac{1}{2}u\left(W_{s_2}-W_\bar{s}\right)\right)\right)
\end{align*}

\(W_\bar{s}-W_{s_1}\) and \(W_{s_2}-W_\bar{s}\) are independant:

\begin{align*}
\mathbb{E}\left(e^{u W_{\bar{s}}} \;|\; W_{s_1} = x_1, W_{s_2} = x_2\right)
&= e^{u \bar{x}} \exp\left(\frac{1}{2} \left(\bar{s}-s_1\right)^2 \left(\frac{u}{2}\right)^2\right) \exp\left(\frac{1}{2} \left(s_2-\bar{s}\right)^2 \left(\frac{u}{2}\right)^2\right) \\
&= e^{u \bar{x}} \exp\left(\frac{u^2}{8} \left[\left(\frac{s_2-s_1}{2}\right)^2 + \left(\frac{s_2-s_1}{2}\right)^2\right]\right) \\
&= e^{u \bar{x}} e^{\frac{u^2}{16}\left(\frac{s_2-s_1}{2}\right)^2}.
\end{align*}

So the variable \(W_\bar{s} \;|\; W_{s_1} = x_1, W_{s_2} = x_2\) is
distributed along a normal distribution \(\mathcal{N}\left(\bar{x},
\frac{s_2-s_1}{4}\right).\)

The steps above also show that the random variable is independant from
\((W_u)_{u\notin [s_1,s_2]}.\)

#+BEGIN_SRC ipython :session :exports both

#+END_SRC

